{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4878e4-248b-4f22-bacc-6389f8b5f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960e34ec-747b-4432-8981-8fa752170df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the ChromeDriver executable\n",
    "chrome_driver_path = '/Users/noahlee/Downloads/chromedriver-mac-arm64/chromedriver'\n",
    "\n",
    "# Initialize the WebDriver service\n",
    "service = Service(executable_path=chrome_driver_path)\n",
    "\n",
    "# Initialize the WebDriver with the service\n",
    "driver = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ba8eaa-12ce-4e6f-b198-37be56b127fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base URL of the AIX Academy website\n",
    "base_url = \"https://www.g-h.store/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b328bff8-1149-4d68-93df-006e75de4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all the URLs\n",
    "urls_to_scrape = [base_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc9ad75-ed28-4684-901a-fb6f80e23ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to store visited URLs\n",
    "visited_urls = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d3d8987-cb18-4632-9407-3eff6c39be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all the links on a page\n",
    "def get_all_links(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    page_links = set()\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        if href.startswith('/'):\n",
    "            href = base_url + href.lstrip('/')\n",
    "        if base_url in href and href not in visited_urls:\n",
    "            if \"/products\" not in href:\n",
    "                page_links.add(href)\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67005a3c-c519-47a2-86d4-d7629ef57eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl the website and collect URLs\n",
    "numpages = len(current_url)\n",
    "while urls_to_scrape and numpages<800:\n",
    "    current_url = urls_to_scrape.pop(0)\n",
    "    if current_url not in visited_urls:\n",
    "        if \"/products\" in current_url:\n",
    "            continue \n",
    "        numpages+=1\n",
    "        visited_urls.add(current_url)\n",
    "        print(current_url)\n",
    "        new_links = get_all_links(current_url)\n",
    "        urls_to_scrape.extend(new_links - visited_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0118418-e9f5-412d-8a03-0c5fa04c1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(visited_urls)\n",
    "import pickle \n",
    "with open('visited_urls.pkl', 'wb') as file:\n",
    "    pickle.dump(visited_urls, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297c96e8-d8f2-4c0f-b10d-1741453165a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the scraped content\n",
    "if not os.path.exists('text'):\n",
    "    os.makedirs('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "257d93d7-3cd4-4550-bd19-fbe9511884f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save page content to a .txt file\n",
    "def save_page_content(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    text_content = soup.get_text(separator='\\n', strip=True)\n",
    "    text_content=text_content[:-385]\n",
    "    file_name = url.replace(base_url, '').replace('/', '_') + '.txt'\n",
    "    file_path = os.path.join('text', file_name)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3479356f-e2ee-4dea-ad4a-ee0f1d7085e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape content from each URL and save it\n",
    "for url in visited_urls:\n",
    "    save_page_content(url)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798123e2-5e52-49ca-bcb1-2b5a34c44a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
